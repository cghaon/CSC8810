# -*- coding: utf-8 -*-
"""Copy of HW1-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hJm_o6G5Q0oXFrtpaYePmmFnk23sf92C
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
tf.logging.set_verbosity(tf.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}



tf.__version__

tf.set_random_seed(1)
np.random.seed(1)

# fake data
x = np.linspace(-1, 1, 100)[:, np.newaxis]          # shape (100, 1)
y = np.sin(5*np.pi*x)/(5*np.pi*x)    
y1 = np.sign(np.sin(5*np.pi*x))  
epoch = 100000

# plot data
plt.scatter(x, y)
plt.show()

input_x = tf.placeholder(tf.float32, [None, 1])      # input x
#input_x = tf.placeholder(tf.float32, [100, 1])     
output_y = tf.placeholder(tf.float32, [None, 1])     # input y
#output_y = tf.placeholder(tf.float32, [100, 1])

"""M0"""

m0h1 = tf.layers.dense(inputs=input_x, units=5, activation=tf.nn.relu, name='m0h1')   # hidden layer
m0h2 = tf.layers.dense(inputs=m0h1, units=10, activation=tf.nn.relu, name='m0h2')        # hidden layer
m0h3 = tf.layers.dense(inputs=m0h2, units=10, activation=tf.nn.relu, name='m0h3')        # hidden layer
m0h4 = tf.layers.dense(inputs=m0h3, units=10, activation=tf.nn.relu, name='m0h4')        # hidden layer
m0h5 = tf.layers.dense(inputs=m0h4, units=10, activation=tf.nn.relu, name='m0h5') 
m0h6 = tf.layers.dense(inputs=m0h5, units=10, activation=tf.nn.relu, name='m0h6') 
m0h7 = tf.layers.dense(inputs=m0h6, units=5, activation=tf.nn.relu, name='m0h7') 
m0output = tf.layers.dense(inputs=m0h7, units=1, name='m0output')                     # output layer

"""M1

M2
"""

loss = tf.losses.mean_squared_error(output_y, m0output)   # compute cost

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)
train_op = optimizer.minimize(loss)

total_parameters = 0
for variable in tf.trainable_variables():
    # shape is an array of tf.Dimension
    print(variable)
    shape = variable.get_shape()
    print(shape)
    #print(len(shape))
    variable_parameters = 1
    for dim in shape:
        #print(dim)
        variable_parameters *= dim.value
    print(variable_parameters)
    total_parameters += variable_parameters
print(total_parameters)

sess = tf.Session()     
sess.run(tf.global_variables_initializer())

#train_op = optimizer.minimize(loss)
# loss = tf.losses.mean_squared_error(output_y, output)
losslist0 = []
for iteration in range(epoch):
    # train and net output
    _, l, pred = sess.run([train_op, loss, m0output], feed_dict={input_x: x, output_y: y})
    losslist0.append(l)
plt.cla()
plt.scatter(x, y)
plt.plot(x, pred, 'r-', lw=5)
plt.text(0.5, 0, 'Loss=%.4f' % l, fontdict={'size': 20, 'color': 'red'})
plt.pause(0.1)
finalpred0 = pred
plt.ioff()
plt.show()

sess = tf.Session()     
sess.run(tf.global_variables_initializer())

#train_op = optimizer.minimize(loss)
# loss = tf.losses.mean_squared_error(output_y, output)
y1losslist0 = []
for iteration in range(epoch):
    # train and net output
    _, l, pred = sess.run([train_op, loss, m0output], feed_dict={input_x: x, output_y: y1})
    y1losslist0.append(l)
plt.cla()
plt.plot(x, y1)
plt.plot(x, pred, 'r-', lw=5)
plt.text(0.5, 0, 'Loss=%.4f' % l, fontdict={'size': 20, 'color': 'red'})
plt.pause(0.1)
y1finalpred0 = pred
plt.ioff()
plt.show()

tf.reset_default_graph()
input_x = tf.placeholder(tf.float32, [None, 1])      # input x
#input_x = tf.placeholder(tf.float32, [100, 1])     
output_y = tf.placeholder(tf.float32, [None, 1])     # input y
#output_y = tf.placeholder(tf.float32, [100, 1])

m1h1 = tf.layers.dense(inputs=input_x, units=10, activation=tf.nn.relu, name='m1h1')   # hidden layer
m1h2 = tf.layers.dense(inputs=m1h1, units=18, activation=tf.nn.relu, name='m1h2')        # hidden layer
m1h3 = tf.layers.dense(inputs=m1h2, units=15, activation=tf.nn.relu, name='m1h3')        # hidden layer
m1h4 = tf.layers.dense(inputs=m1h3, units=4, activation=tf.nn.relu, name='m1h4')        # hidden layer 
m1output = tf.layers.dense(inputs=m1h4, units=1, name='m1output')                     # output layer

loss1 = tf.losses.mean_squared_error(output_y, m1output)   # compute cost
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)
train_op1 = optimizer.minimize(loss1)

total_parameters = 0
for variable in tf.trainable_variables():
    # shape is an array of tf.Dimension
    print(variable)
    shape = variable.get_shape()
    print(shape)
    #print(len(shape))
    variable_parameters = 1
    for dim in shape:
        #print(dim)
        variable_parameters *= dim.value
    print(variable_parameters)
    total_parameters += variable_parameters
print(total_parameters)

sess = tf.Session()

sess.run(tf.global_variables_initializer())

#train_op = optimizer.minimize(loss)
# loss = tf.losses.mean_squared_error(output_y, output)
losslist1 = []
for iteration in range(epoch):
    # train and net output
    _, l, pred = sess.run([train_op1, loss1, m1output], feed_dict={input_x: x, output_y: y})
    losslist1.append(l)
plt.cla()
plt.scatter(x, y)
plt.plot(x, pred, 'r-', lw=5)
plt.text(0.5, 0, 'Loss=%.4f' % l, fontdict={'size': 20, 'color': 'red'})
plt.pause(0.1)
finalpred1 = pred
plt.ioff()
plt.show()

sess = tf.Session()     
sess.run(tf.global_variables_initializer())

#train_op = optimizer.minimize(loss)
# loss = tf.losses.mean_squared_error(output_y, output)
y1losslist1 = []
for iteration in range(epoch):
    # train and net output
    _, l, pred = sess.run([train_op1, loss1, m1output], feed_dict={input_x: x, output_y: y1})
    y1losslist1.append(l)
plt.cla()
plt.plot(x, y1)
plt.plot(x, pred, 'r-', lw=5)
plt.text(0.5, 0, 'Loss=%.4f' % l, fontdict={'size': 20, 'color': 'red'})
plt.pause(0.1)
y1finalpred1 = pred
plt.ioff()
plt.show()

tf.reset_default_graph()
input_x = tf.placeholder(tf.float32, [None, 1])      # input x
#input_x = tf.placeholder(tf.float32, [100, 1])     
output_y = tf.placeholder(tf.float32, [None, 1])     # input y
#output_y = tf.placeholder(tf.float32, [100, 1])

input_x = tf.placeholder(tf.float32, [None, 1])      # input x
#input_x = tf.placeholder(tf.float32, [100, 1])     
output_y = tf.placeholder(tf.float32, [None, 1])     # input y
#output_y = tf.placeholder(tf.float32, [100, 1])

m2h1 = tf.layers.dense(inputs=input_x, units=190, activation=tf.nn.relu, name='m2h1')   # hidden layer
m2output = tf.layers.dense(inputs=m2h1, units=1, name='m2output')

loss2 = tf.losses.mean_squared_error(output_y, m2output)   # compute cost
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)
train_op2 = optimizer.minimize(loss2)

sess = tf.Session()

sess.run(tf.global_variables_initializer())

#train_op = optimizer.minimize(loss)
# loss = tf.losses.mean_squared_error(output_y, output)
losslist2 = []
for iteration in range(epoch):
    # train and net output
    _, l, pred = sess.run([train_op2, loss2, m2output], feed_dict={input_x: x, output_y: y})
    losslist2.append(l)
plt.cla()
plt.scatter(x, y)
plt.plot(x, pred, 'r-', lw=5)
plt.text(0.5, 0, 'Loss=%.4f' % l, fontdict={'size': 20, 'color': 'red'})
plt.pause(0.1)
finalpred2 = pred
plt.ioff()
plt.show()

sess = tf.Session()     
sess.run(tf.global_variables_initializer())

#train_op = optimizer.minimize(loss)
# loss = tf.losses.mean_squared_error(output_y, output)
y1losslist2 = []
for iteration in range(epoch):
    # train and net output
    _, l, pred = sess.run([train_op2, loss2, m2output], feed_dict={input_x: x, output_y: y1})
    y1losslist2.append(l)
plt.cla()
plt.plot(x, y1)
plt.plot(x, pred, 'r-', lw=5)
plt.text(0.5, 0, 'Loss=%.4f' % l, fontdict={'size': 20, 'color': 'red'})
plt.pause(0.1)
y1finalpred2 = pred
plt.ioff()
plt.show()

# Function 1
fig,axs = plt.subplots(1,2)
fig.set_figwidth(15)
axs[0].plot(x,finalpred0,x,finalpred1,x,finalpred2,x,y)
fig.suptitle('Function 1')
axs[0].legend(('Model 0','Model 1','Model2','Original'))
axs[0].set_xlabel('x')
axs[0].set_ylabel('f(x)')
epoch_list = np.arange(epoch)
axs[1].plot(epoch_list,losslist0,epoch_list,losslist1,epoch_list,losslist2)
axs[1].legend(('Model 0','Model 1','Model 2'))
axs[1].set_xlabel('Epoch')
axs[1].set_ylabel('Loss');
plt.pause(0.1)
# Function 2
fig,axs = plt.subplots(1,2)
fig.set_figwidth(15)
axs[0].plot(x,y1finalpred0,x,y1finalpred1,x,y1finalpred2,x,y1)
fig.suptitle('Function 2')
axs[0].legend(('Model 0','Model 1','Model2','orginal'))
axs[0].set_xlabel('x')
axs[0].set_ylabel('f(x)')
epoch_list = np.arange(epoch)
axs[1].plot(epoch_list,y1losslist0,epoch_list,y1losslist1,epoch_list,y1losslist2)
axs[1].legend(('Model 0','Model 1','Model 2'))
axs[1].set_xlabel('Epoch')
axs[1].set_ylabel('Loss');
plt.pause(0.1)